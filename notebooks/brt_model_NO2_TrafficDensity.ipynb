{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Imports__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from aqmd_pylib.aqmd_lib import data_toolkit as dtk\n",
    "from aqmd_pylib.aqmd_lib.util import nLoop\n",
    "from aqmd_pylib.aqmd_lib import util"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Settings__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "RANDOM_STATE = None\n",
    "SAVE_FIGURES = True\n",
    "LOAD_FROM_MEMORY = False\n",
    "USE_SCALED = False\n",
    "USE_ONLY_CONTINUOUS = True\n",
    "PARALLEL_CORES = -1  # \"-1\" will use all cores available\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Variables and Filepaths__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "site = 'Iowa'\n",
    "site_filename = 'iowa'\n",
    "traffic_feature = 'TrafficDensity'\n",
    "traffic_filename = 'trafficDensity'\n",
    "traffic_FolderName = 'traffic_density'\n",
    "\n",
    "y_var = 'NO2'\n",
    "y_fileLabel = 'no2'\n",
    "y_units = '[ppb]'\n",
    "\n",
    "resolution = 'HighRes'# 'HighRes' or 'Hourly'\n",
    "\n",
    "filepath_site_pollutant = f'../data/complete_sets/{site_filename}/{site_filename}_full_{y_fileLabel}_{traffic_feature}.csv'\n",
    "filepath_site_x = f'../data/complete_sets/{site_filename}_{y_fileLabel}_{traffic_feature}_x.csv'\n",
    "filepath_site_y = f'../data/complete_sets/{site_filename}_{y_fileLabel}_{traffic_feature}_y.csv'\n",
    "filepath_site_train_x = f'../data/complete_sets/train/{y_fileLabel}_{traffic_feature}_train_x.csv'\n",
    "filepath_site_train_y = f'../data/complete_sets/train/{y_fileLabel}_{traffic_feature}_train_y.csv'\n",
    "filepath_site_test_x = f'../data/complete_sets/test/{y_fileLabel}_{traffic_feature}_test_x.csv'\n",
    "filepath_site_test_y = f'../data/complete_sets/test/{y_fileLabel}_{traffic_feature}_test_y.csv'\n",
    "\n",
    "FolderPath_graphs = f'../graphs/{y_fileLabel}/{traffic_FolderName}'\n",
    "\n",
    "datetime_format = '%Y-%m-%d %H:%M:%S%z'\n",
    "\n",
    "y_full_label = f'{site} {y_var} {y_units}'\n",
    "continuous_x = [\n",
    "    'Temperature [degC]',\n",
    "    'Pressure [mbar]',\n",
    "    'Humidity [%]',\n",
    "    'Wind Speed [mph]',\n",
    "    'Wind Direction [degrees]',\n",
    "    f'Background {y_var} {y_units}',\n",
    "    'Density Local 1 (#Vehicles/mile)',\n",
    "    'Density Local 2 (#Vehicles/mile)',\n",
    "    'Density Local 9 (#Vehicles/mile)',\n",
    "    'Density Local 10 (#Vehicles/mile)',\n",
    "    'Density Fwy 1 (#Vehicles/mile)',\n",
    "    'Density Fwy 4 (#Vehicles/mile)'\n",
    "]\n",
    "continous_vars = continuous_x.append(y_full_label)\n",
    "date_fileNames=['year', 'month', 'day', 'dayofweek_01_Sunday', 'dayofweek_02_Monday', 'dayofweek_03_Tuesday',\n",
    "             'dayofweek_04_Wednesday', 'dayofweek_05_Thursday', 'dayofweek_06_Friday', 'dayofweek_07_Saturday']\n",
    "feat_fileNames = ['temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'background_pm25', f'{traffic_filename}',\n",
    "                      f'{traffic_filename}_local2', f'{traffic_filename}_local9', f'{traffic_filename}_local10',\n",
    "                      f'{traffic_filename}_fwy1', f'{traffic_filename}']\n",
    "if ~USE_ONLY_CONTINUOUS:\n",
    "    feat_fileNames= date_fileNames + feat_fileNames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Load Data__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/complete_sets/iowa/iowa_full_no2_TrafficDensity.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_34432/4162193861.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;33m~\u001B[0m\u001B[0mLOAD_FROM_MEMORY\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0msite_pollutant\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_site_pollutant\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     site_pollutant = dtk.df_str2dt(site_pollutant, 'datetime-America/Los_Angeles',\n\u001B[0;32m      4\u001B[0m                                    datetime_format, curr_tz='America/Los_Angeles', overwrite=True)\n\u001B[0;32m      5\u001B[0m     \u001B[0msite_pollutant\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'datetime-America/Los_Angeles'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 482\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    483\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    810\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 811\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    812\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1038\u001B[0m             )\n\u001B[0;32m   1039\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1040\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \"\"\"\n\u001B[1;32m--> 222\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    700\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    701\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 702\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/complete_sets/iowa/iowa_full_no2_TrafficDensity.csv'"
     ]
    }
   ],
   "source": [
    "if ~LOAD_FROM_MEMORY:\n",
    "    site_pollutant = pd.read_csv(filepath_site_pollutant)\n",
    "    site_pollutant = dtk.df_str2dt(site_pollutant, 'datetime-America/Los_Angeles',\n",
    "                                   datetime_format, curr_tz='America/Los_Angeles', overwrite=True)\n",
    "    site_pollutant.set_index('datetime-America/Los_Angeles', inplace=True)\n",
    "    if USE_ONLY_CONTINUOUS:\n",
    "        site_pollutant = site_pollutant.loc[:, continuous_x]\n",
    "        site_scaled = site_pollutant.copy()\n",
    "    for feat in continuous_x:\n",
    "        site_scaled[feat] = StandardScaler().fit_transform(site_scaled.loc[:, [feat]])\n",
    "    if USE_SCALED:\n",
    "        site_x = site_scaled.drop(columns=y_full_label)\n",
    "    else:\n",
    "        site_x = site_pollutant.drop(columns=y_full_label)\n",
    "    site_y = site_pollutant.loc[:, y_full_label]\n",
    "    site_train_x, site_test_x, site_train_y, site_test_y = train_test_split(\n",
    "        site_x, site_y,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    site_x.to_csv(filepath_site_x)\n",
    "    site_y.to_csv(filepath_site_y)\n",
    "    site_train_x.to_csv(filepath_site_train_x)\n",
    "    site_train_y.to_csv(filepath_site_train_y)\n",
    "    site_test_x.to_csv(filepath_site_test_x)\n",
    "    site_test_y.to_csv(filepath_site_test_y)\n",
    "else:\n",
    "    site_x = pd.read_csv(filepath_site_x)\n",
    "    site_x = dtk.df_str2dt(site_x, 'datetime-America/Los_Angeles', datetime_format)\n",
    "    site_y = pd.read_csv(filepath_site_y)\n",
    "    site_y = dtk.df_str2dt(site_y, 'datetime-America/Los_Angeles', datetime_format)\n",
    "    site_train_x = pd.read_csv(filepath_site_train_x)\n",
    "    site_train_x = dtk.df_str2dt(site_train_x, 'datetime-America/Los_Angeles', datetime_format)\n",
    "    site_train_y = pd.read_csv(filepath_site_train_y)\n",
    "    site_train_y = dtk.df_str2dt(site_train_y, 'datetime-America/Los_Angeles', datetime_format)\n",
    "    site_test_x = pd.read_csv(filepath_site_test_x)\n",
    "    site_test_x = dtk.df_str2dt(site_test_x, 'datetime-America/Los_Angeles', datetime_format)\n",
    "    site_test_y = pd.read_csv(filepath_site_test_y)\n",
    "    site_test_y = dtk.df_str2dt(site_test_y, 'datetime-America/Los_Angeles', datetime_format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Gradient Boosted Regression Model__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "> ## __Model Parameters__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbt_model_params = {\n",
    "    'loss'                    : 'huber',\n",
    "    'learning_rate'           : 0.001,\n",
    "    'verbose'                 : True,\n",
    "    'n_estimators'            : 5000,\n",
    "    'subsample'               : 1.0,\n",
    "    'criterion'               : 'friedman_mse',\n",
    "    'min_samples_split'       : 2,\n",
    "    'min_samples_leaf'        : 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_depth'               : 4,\n",
    "    'min_impurity_decrease'   : 0.0,\n",
    "    'init'                    : None,\n",
    "    'random_state'            : RANDOM_STATE,\n",
    "    'max_features'            : 'auto',\n",
    "    'alpha'                   : 0.9,\n",
    "    'max_leaf_nodes'          : None,\n",
    "    'warm_start'              : False,\n",
    "    'validation_fraction'     : 0.1,\n",
    "    'n_iter_no_change'        : 100\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "> ## __Cross Validation - Model Evaluation__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbt_model_cv =  GradientBoostingRegressor(**gbt_model_params)\n",
    "cv_split = KFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "cv_results = cross_validate(gbt_model_cv,\n",
    "                            X=site_x,\n",
    "                            y=site_y,\n",
    "                            n_jobs=PARALLEL_CORES,\n",
    "                            cv=cv_split,\n",
    "                            scoring=(\n",
    "                                'r2', 'explained_variance', 'neg_mean_squared_error', 'neg_root_mean_squared_error'),\n",
    "                            return_train_score=True,\n",
    "                            verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cv_results.keys())\n",
    "score_cols = ['Fit Time:', 'Score Time:', 'Avg. Test R2:', 'Avg. Test MSE:', 'Avg. Test RMSE:',\n",
    "              'Avg. Test Explained Variance:',\n",
    "              'Avg. Train R2:', 'Avg. Train MSE:', 'Avg. Train RMSE:', 'Avg. Train Explained Variance:']\n",
    "\n",
    "score_matrix = [[mean(cv_results['fit_time']), mean(cv_results['score_time']), mean(cv_results['test_r2']),\n",
    "                 abs(mean(cv_results['test_neg_mean_squared_error'])),\n",
    "                 abs(mean(cv_results['test_neg_root_mean_squared_error'])),\n",
    "                 mean(cv_results['test_explained_variance']), mean(cv_results['train_r2']),\n",
    "                 abs(mean(cv_results['train_neg_mean_squared_error'])),\n",
    "                 abs(mean(cv_results['train_neg_root_mean_squared_error'])),\n",
    "                 mean(cv_results['train_explained_variance'])]]\n",
    "print(\n",
    "    f'\\nCross Validation Results:\\n'\n",
    "    f'  Fit Time: {mean(cv_results[\"fit_time\"])}\\n'\n",
    "    f'  Score Time: {mean(cv_results[\"score_time\"])}\\n'\n",
    "    f'  Test R2: {mean(cv_results[\"test_r2\"])}\\n'\n",
    "    f'  Train R2: {mean(cv_results[\"train_r2\"])}\\n'\n",
    "    f'  Test MSE: {abs(mean(cv_results[\"test_neg_mean_squared_error\"]))}\\n'\n",
    "    f'  Train MSE: {abs(mean(cv_results[\"train_neg_mean_squared_error\"]))}\\n'\n",
    "    f'  Test RMSE: {abs(mean(cv_results[\"test_neg_root_mean_squared_error\"]))}\\n'\n",
    "    f'  Train RMSE: {abs(mean(cv_results[\"train_neg_root_mean_squared_error\"]))}\\n'\n",
    "    f'  Test Explained Variance: {mean(cv_results[\"test_explained_variance\"])}\\n'\n",
    "    f'  Train Explained Variance: {mean(cv_results[\"train_explained_variance\"])}\\n'\n",
    ")\n",
    "score_table = pd.DataFrame(data=score_matrix, columns=score_cols).melt(var_name='Cross Validation Metric',\n",
    "                                                                       value_name='Score Results')\n",
    "score_table['Score Results'] = score_table['Score Results'].apply(lambda x: round(x, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "score_fig, score_ax = plt.subplots(dpi=300)\n",
    "score_fig.suptitle('Cross Validation Scores')\n",
    "color_matrix = np.full((10, 2), fill_value='gainsboro')\n",
    "score_ax.set_axis_off()\n",
    "score_ax.table(cellColours=color_matrix, cellText=score_table.values, cellLoc='left',\n",
    "               colLabels=['Model Metrics:', 'Scores'], bbox=[0.1, 0.1, 0.8, 0.8])\n",
    "if SAVE_FIGURES:\n",
    "    score_fig.savefig(FolderPath_graphs + '/cross_validation_scores.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbt_pipeline_cvp = make_pipeline(StandardScaler(), GradientBoostingRegressor(**gbt_model_params))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_y = cross_val_predict(gbt_pipeline_cvp,\n",
    "                         X=site_x,\n",
    "                         y=site_y,\n",
    "                         cv=cv_split,\n",
    "                         n_jobs=PARALLEL_CORES,\n",
    "                         verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_fig, cv_ax = plt.subplots(dpi=300, figsize=(8, 4))\n",
    "cv_site = pd.concat([site_x, site_y], axis=1)\n",
    "cv_site[f'Predicted {site} {y_var} {y_units}'] = cv_y\n",
    "cv_site.sort_index(inplace=True)\n",
    "cv_site.reset_index(inplace=True)\n",
    "cv_ax.scatter(\n",
    "    cv_site.loc[:, 'datetime-America/Los_Angeles'],\n",
    "    cv_site.loc[:, f'{site} {y_var} {y_units}'],\n",
    "    label=f'Observed {y_var}',\n",
    "    alpha=0.3\n",
    ")\n",
    "cv_ax.plot(\n",
    "    cv_site.loc[:, 'datetime-America/Los_Angeles'],\n",
    "    cv_site.loc[:, f'Predicted {site} {y_var} {y_units}'],\n",
    "    label=f'Predicted {y_var} {y_units}',\n",
    "    color='red'\n",
    ")\n",
    "cv_fig.suptitle(f'Cross Validation - Predicted vs Observed {site} {y_var} {y_units}')\n",
    "cv_ax.legend()\n",
    "cv_ax.grid()\n",
    "if SAVE_FIGURES:\n",
    "    cv_fig.savefig(FolderPath_graphs + f'/cross_validation_{y_fileLabel}.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "> ## __Leave One Out - Model Evaluation__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbt_model_loo = GradientBoostingRegressor(**gbt_model_params)\n",
    "gbt_pipeline_loo=make_pipeline(StandardScaler(),GradientBoostingRegressor(**gbt_model_params))\n",
    "gbt_model_loo.fit(X=site_train_x, y=site_train_y)\n",
    "\n",
    "iowa_test_predY = gbt_model_loo.predict(site_test_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "iowa_test = pd.concat([site_test_x, site_test_y], axis=1)\n",
    "iowa_test[f'pred {site} {y_var} {y_units}'] = iowa_test_predY\n",
    "iowa_test.sort_index(inplace=True)\n",
    "iowa_test.reset_index(inplace=True)\n",
    "iowa_test.info()\n",
    "iowa_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\n",
    "    f'\\nLeave One Out Results:\\n'\n",
    "    f'  R2: {metrics.r2_score(site_test_y, iowa_test_predY)}\\n'\n",
    "    f'  MSE:{metrics.mean_squared_error(site_test_y, iowa_test_predY, squared=True)}\\n'\n",
    "    f'  RMSE: {metrics.mean_squared_error(site_test_y, iowa_test_predY, squared=False)}\\n'\n",
    "    f'  Explained Variance {metrics.explained_variance_score(site_test_y, iowa_test_predY)}\\n'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_fig, train_ax = plt.subplots(dpi=300, figsize=(8, 4))\n",
    "site_train_x.sort_index(inplace=True)\n",
    "train_ax.plot(\n",
    "    site_train_x.index.values,\n",
    "    gbt_model_loo.predict(site_train_x),\n",
    "    color='red',\n",
    "    label=f'pred {site} {y_var} {y_units}'\n",
    ")\n",
    "train_ax.scatter(\n",
    "    site_train_y.index.values,\n",
    "    site_train_y,\n",
    "    alpha=0.3,\n",
    "    label=f'{site} {y_var} {y_units}'\n",
    ")\n",
    "train_fig.suptitle(f'Training Set-{site} {y_var}')\n",
    "train_ax.grid(linestyle=':')\n",
    "train_ax.legend()\n",
    "if SAVE_FIGURES:\n",
    "    train_fig.savefig(FolderPath_graphs + f'/{site}_{y_var}_Training_Data.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_fig, valid_ax = plt.subplots(dpi=300, figsize=(8, 4))\n",
    "valid_ax.plot(\n",
    "    iowa_test.loc[:, 'datetime-America/Los_Angeles'],\n",
    "    iowa_test.loc[:, f'pred {site} {y_var} {y_units}'],\n",
    "    color='red',\n",
    "    label=f'pred {site} {y_var} {y_units}'\n",
    ")\n",
    "valid_ax.scatter(\n",
    "    iowa_test.loc[:, 'datetime-America/Los_Angeles'],\n",
    "    iowa_test.loc[:, f'{site} {y_var} {y_units}'],\n",
    "    alpha=0.5,\n",
    "    color='blue',\n",
    "    label=f'{site} {y_var} {y_units}'\n",
    ")\n",
    "valid_ax.set_ylabel(f'{y_var} Concentration {y_units}')\n",
    "valid_ax.set_xlabel('Date & Time - America/Los_Angeles')\n",
    "# valid_ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=1))\n",
    "# valid_ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%y'))\n",
    "valid_ax.grid(axis='both', alpha=0.1, color='black', linestyle=':')\n",
    "valid_fig.suptitle(f'Test Set Predicted {y_var} vs. Observed {y_var} {y_units}')\n",
    "valid_ax.legend()\n",
    "if SAVE_FIGURES:\n",
    "    valid_fig.savefig(FolderPath_graphs + f'/{site}_{y_var}_Test_Evaluation.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# __Partial Dependence__\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dtk.df_getColNames(site_train_x))\n",
    "site_train_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    gbt_model_loo,\n",
    "    X=site_train_x,\n",
    "    features=nLoop(dtk.df_getColNames(site_train_x)),\n",
    "    feature_names=dtk.df_getColNames(site_train_x),\n",
    "    verbose=True,\n",
    "    n_jobs=PARALLEL_CORES,\n",
    "    method='auto',\n",
    "    kind='average',\n",
    "    random_state=RANDOM_STATE,\n",
    "    grid_resolution=150,\n",
    "    subsample=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "partial_fig = []\n",
    "partial_ax = []\n",
    "for n in nLoop(dtk.df_getColNames(site_train_x)):\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    plt.close(fig)\n",
    "    partial_fig.append(fig)\n",
    "    partial_ax.append(ax)\n",
    "display.plot(ax=partial_ax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "n_rows=int(len(feat_fileNames)/2)\n",
    "full_partial_fig, full_partial_ax = plt.subplots(nrows=n_rows, ncols=2, dpi=300, figsize=(22, 44))\n",
    "display.plot(ax=full_partial_ax)\n",
    "\n",
    "\n",
    "def activate_grid(x):\n",
    "    x.grid(linestyle=':')\n",
    "\n",
    "\n",
    "activate_grid_v = np.vectorize(activate_grid)\n",
    "activate_grid_v(full_partial_ax)\n",
    "full_partial_fig.savefig(FolderPath_graphs + f'/{y_var}_full_partial_dependence.png', dpi=300, facecolor='white',\n",
    "                         edgecolor='azure')\n",
    "full_partial_fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "partial_feats = dtk.df_getColNames(site_train_x)\n",
    "\n",
    "for i in nLoop(partial_fig):\n",
    "    partial_fig[i].suptitle(f'Partial Dependence - {partial_feats[i]}')\n",
    "    partial_ax[i].grid(linestyle=':')\n",
    "    if SAVE_FIGURES:\n",
    "        partial_fig[i].savefig(\n",
    "            FolderPath_graphs + f'/partial_dependence/{y_var}_partial_dependence_{feat_fileNames[i]}.png',\n",
    "            dpi=300,\n",
    "            facecolor='white',\n",
    "            edgecolor='azure'\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}